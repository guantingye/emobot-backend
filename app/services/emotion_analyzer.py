# backend/app/services/emotion_analyzer.py - é–€æª»èª¿æ•´è‡³30å¥
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
from collections import Counter, defaultdict
import re
import math

EMOTION_KEYWORDS = {
    "ç„¦æ…®ä¸å®‰": {
        "keywords": ["æ“”å¿ƒ", "å®³æ€•", "ç·Šå¼µ", "ç„¦æ…®", "ä¸å®‰", "ææ‡¼", "é©šæ…Œ", "ç…©èº", "å¿å¿‘", "æƒ¶æ"],
        "weight": 1.2,
        "color": "#FF6B6B"
    },
    "æ†‚é¬±ä½è½": {
        "keywords": ["é›£é", "æ†‚é¬±", "ä½è½", "æ²®å–ª", "å¤±è½", "æ‚²å‚·", "ç—›è‹¦", "çµ•æœ›", "å­¤å–®", "ç©ºè™›"],
        "weight": 1.2,
        "color": "#4ECDC4"
    },
    "æ†¤æ€’ç…©èº": {
        "keywords": ["ç”Ÿæ°£", "æ†¤æ€’", "ç«å¤§", "ä¸çˆ½", "ç…©", "è¨å­", "æ°£", "æ¨", "æŠ“ç‹‚"],
        "weight": 1.1,
        "color": "#FF8C42"
    },
    "å£“åŠ›ç–²æ†Š": {
        "keywords": ["å£“åŠ›", "ç´¯", "ç–²æ†Š", "å¿™", "è¶•", "çˆ†ç‚¸", "å—ä¸äº†", "æ’ä¸ä½", "å´©æ½°"],
        "weight": 1.15,
        "color": "#95E1D3"
    },
    "å¿«æ¨‚æ»¿è¶³": {
        "keywords": ["é–‹å¿ƒ", "å¿«æ¨‚", "é«˜èˆˆ", "å–œæ­¡", "æ£’", "å¥½", "è®š", "çˆ½", "èˆˆå¥®", "æ»¿è¶³"],
        "weight": 0.9,
        "color": "#FEE440"
    },
    "å¹³éœæ”¾é¬†": {
        "keywords": ["å¹³éœ", "æ”¾é¬†", "èˆ’æœ", "å®‰å¿ƒ", "ç©©å®š", "é‚„å¥½", "æ²’äº‹", "æ·¡å®š", "å¾å®¹"],
        "weight": 0.8,
        "color": "#38B6FF"
    },
    "å›°æƒ‘è¿·èŒ«": {
        "keywords": ["å›°æƒ‘", "è¿·èŒ«", "ä¸çŸ¥é“", "çŒ¶è±«", "ç³¾çµ", "çŸ›ç›¾", "ç–‘æƒ‘", "è¿·å¤±"],
        "weight": 1.0,
        "color": "#B983FF"
    }
}

TOPIC_KEYWORDS = {
    "äººéš›é—œä¿‚": {
        "keywords": ["æœ‹å‹", "å®¶äºº", "åŒäº‹", "é—œä¿‚", "ç›¸è™•", "åµæ¶", "æºé€š", "å­¤å–®", "ç¤¾äº¤", "äººéš›"],
        "weight": 1.0,
        "color": "#FF6B9D"
    },
    "å·¥ä½œè·å ´": {
        "keywords": ["å·¥ä½œ", "ä¸Šç­", "è€é—†", "åŒäº‹", "åŠ ç­", "å°ˆæ¡ˆ", "æ¥­ç¸¾", "å ±å‘Š", "è·å ´", "å·¥ä½œ"],
        "weight": 1.1,
        "color": "#5B8DEE"
    },
    "å­¸æ¥­æˆé•·": {
        "keywords": ["è€ƒè©¦", "æˆç¸¾", "å­¸æ ¡", "è€å¸«", "ä½œæ¥­", "å ±å‘Š", "è®€æ›¸", "èª²æ¥­", "å­¸ç¿’", "é€²ä¿®"],
        "weight": 1.0,
        "color": "#00D9FF"
    },
    "æƒ…æ„Ÿæ„›æƒ…": {
        "keywords": ["æ„Ÿæƒ…", "æˆ€æ„›", "åˆ†æ‰‹", "å–œæ­¡", "æ›–æ˜§", "å‘Šç™½", "å¤±æˆ€", "äº¤å¾€", "ä¼´ä¾¶", "å–œæ­¡"],
        "weight": 1.2,
        "color": "#FF85A2"
    },
    "è‡ªæˆ‘èªåŒ": {
        "keywords": ["è‡ªå·±", "è‡ªæˆ‘", "åƒ¹å€¼", "æ„ç¾©", "è¿·æƒ˜", "ç‚ºä»€éº¼", "èº«åˆ†", "äººç”Ÿ", "æœªä¾†", "ç›®æ¨™"],
        "weight": 1.15,
        "color": "#A8E6CF"
    },
    "èº«å¿ƒå¥åº·": {
        "keywords": ["èº«é«”", "å¥åº·", "ç”Ÿç—…", "ç—›", "ä¸èˆ’æœ", "ç¡ä¸è‘—", "å¤±çœ ", "é ­ç—›", "ç–²å‹", "ç„¦æ…®"],
        "weight": 1.2,
        "color": "#FFB6B9"
    }
}

INTENSITY_MODIFIERS = {
    "æ¥µå¼·": {
        "words": ["éå¸¸éå¸¸", "è¶…ç´šè¶…ç´š", "æ¥µåº¦", "å®Œå…¨", "çµ•å°"],
        "multiplier": 1.5
    },
    "å¼·": {
        "words": ["éå¸¸", "è¶…ç´š", "çœŸçš„å¾ˆ", "å¤ª", "ç‰¹åˆ¥", "ç›¸ç•¶"],
        "multiplier": 1.3
    },
    "ä¸­å¼·": {
        "words": ["å¾ˆ", "è »", "æŒº", "ååˆ†", "é —"],
        "multiplier": 1.1
    },
    "ä¸­": {
        "words": ["æœ‰é»", "é‚„ç®—", "ç¨å¾®", "äº›è¨±", "ç®—æ˜¯"],
        "multiplier": 0.9
    },
    "å¼±": {
        "words": ["ä¸€é»é»", "ä¸å¤ª", "å¾®å¾®", "ä¼¼ä¹", "å¥½åƒ"],
        "multiplier": 0.7
    }
}

def time_decay_weight(days_ago: int) -> float:
    """è¨ˆç®—æ™‚é–“è¡°æ¸›æ¬Šé‡"""
    return math.exp(-0.05 * days_ago)

def calculate_emotion_intensity(text: str, base_score: float) -> float:
    """è¨ˆç®—æƒ…ç·’å¼·åº¦"""
    intensity = base_score
    
    for level_data in INTENSITY_MODIFIERS.values():
        for word in level_data["words"]:
            if word in text:
                intensity *= level_data["multiplier"]
                break
    
    exclamation_count = text.count("ï¼") + text.count("!")
    question_count = text.count("ï¼Ÿï¼Ÿ") + text.count("??")
    
    intensity *= (1 + exclamation_count * 0.1)
    intensity *= (1 + question_count * 0.08)
    
    if re.search(r'[A-Z]{3,}', text):
        intensity *= 1.15
    
    return min(intensity, 1.0)

def detect_emotions_advanced(text: str, days_ago: int = 0) -> Dict[str, float]:
    """é€²éšæƒ…ç·’åµæ¸¬"""
    emotions = {}
    
    for emotion, data in EMOTION_KEYWORDS.items():
        keywords = data["keywords"]
        weight = data["weight"]
        
        count = sum(text.count(word) for word in keywords)
        
        if count > 0:
            base_score = min(count * 0.15, 0.8)
            intensity = calculate_emotion_intensity(text, base_score)
            weighted_score = intensity * weight
            time_weighted = weighted_score * time_decay_weight(days_ago)
            emotions[emotion] = time_weighted
    
    return emotions

def detect_topics_advanced(text: str) -> Dict[str, float]:
    """é€²éšè­°é¡Œåµæ¸¬"""
    topics = {}
    
    for topic, data in TOPIC_KEYWORDS.items():
        keywords = data["keywords"]
        weight = data["weight"]
        
        count = sum(text.count(word) for word in keywords)
        
        if count > 0:
            score = min(count * 0.2, 0.9) * weight
            topics[topic] = score
    
    return topics

def analyze_emotion_trends(timeline_data: List[Dict]) -> Dict[str, Any]:
    """åˆ†ææƒ…ç·’è¶¨å‹¢è®ŠåŒ–"""
    if len(timeline_data) < 5:
        return {"trend": "insufficient_data"}
    
    recent_emotions = [d["dominant_emotion"] for d in timeline_data[-10:]]
    emotion_counter = Counter(recent_emotions)
    
    negative_emotions = ["ç„¦æ…®ä¸å®‰", "æ†‚é¬±ä½è½", "æ†¤æ€’ç…©èº", "å£“åŠ›ç–²æ†Š"]
    negative_count = sum(1 for e in recent_emotions if e in negative_emotions)
    
    if negative_count > len(recent_emotions) * 0.7:
        trend = "concerning"
        trend_description = "è¿‘æœŸè² é¢æƒ…ç·’è¼ƒå¤šï¼Œå»ºè­°å°‹æ±‚æ”¯æŒ"
    elif negative_count > len(recent_emotions) * 0.4:
        trend = "fluctuating"
        trend_description = "æƒ…ç·’èµ·ä¼è¼ƒå¤§ï¼Œæ³¨æ„è‡ªæˆ‘èª¿é©"
    else:
        trend = "stable"
        trend_description = "æƒ…ç·’ç‹€æ…‹ç›¸å°ç©©å®š"
    
    return {
        "trend": trend,
        "description": trend_description,
        "dominant_emotions": dict(emotion_counter.most_common(3))
    }

def generate_professional_summary(
    emotion_freq: Dict[str, int],
    emotion_intensity: Dict[str, float],
    topics: Dict[str, float],
    message_count: int,
    trend_info: Dict[str, Any]
) -> str:
    """ç”Ÿæˆå°ˆæ¥­çš„å¿ƒç†åˆ†ææ‘˜è¦"""
    
    if message_count < 30:
        return f"ç›®å‰å°è©±æ¬¡æ•¸ç‚º {message_count} æ¬¡ï¼Œå»ºè­°ç´¯ç©è‡³å°‘ 30 æ¬¡å°è©±å¾Œå†é€²è¡Œå®Œæ•´åˆ†æï¼Œä»¥ç²å¾—æ›´æº–ç¢ºçš„å¿ƒç†ç‹€æ…‹è©•ä¼°ã€‚"
    
    summary_parts = []
    
    if emotion_freq:
        top_emotion = max(emotion_freq.items(), key=lambda x: x[1])[0]
        freq_count = emotion_freq[top_emotion]
        
        summary_parts.append(f"ğŸ“Š æƒ…ç·’ç‰¹å¾µåˆ†æï¼š")
        summary_parts.append(f"åœ¨ {message_count} æ¬¡å°è©±ä¸­ï¼Œä½ æœ€å¸¸è¡¨é”ã€Œ{top_emotion}ã€çš„æƒ…ç·’ï¼ˆå‡ºç¾ {freq_count} æ¬¡ï¼‰")
        
        if top_emotion == "ç„¦æ…®ä¸å®‰":
            summary_parts.append("ğŸ’¡ å»ºè­°ï¼šå˜—è©¦æ­£å¿µå‘¼å¸æ³•æˆ–æ¼¸é€²å¼è‚Œè‚‰æ”¾é¬†ï¼Œæœ‰åŠ©æ–¼é™ä½ç„¦æ…®æ„Ÿ")
        elif top_emotion == "æ†‚é¬±ä½è½":
            summary_parts.append("ğŸ’¡ å»ºè­°ï¼šæŒçºŒçš„ä½è½æƒ…ç·’éœ€è¦é—œæ³¨ï¼Œå»ºè­°èˆ‡å°ˆæ¥­å¿ƒç†è«®å•†å¸«è¨è«–")
        elif top_emotion == "å£“åŠ›ç–²æ†Š":
            summary_parts.append("ğŸ’¡ å»ºè­°ï¼šé©åº¦ä¼‘æ¯å’Œè¦å¾‹é‹å‹•å¯ä»¥æœ‰æ•ˆé‡‹æ”¾å£“åŠ›ï¼Œä¹Ÿå¯å˜—è©¦æ™‚é–“ç®¡ç†æŠ€å·§")
        elif top_emotion == "å¿«æ¨‚æ»¿è¶³":
            summary_parts.append("ğŸ’¡ å¾ˆæ£’ï¼ä½ çš„æ­£å‘æƒ…ç·’è¡¨é”é »ç‡è¼ƒé«˜ï¼Œç¹¼çºŒä¿æŒé€™æ¨£çš„ç‹€æ…‹")
    
    if topics:
        top_topic = max(topics.items(), key=lambda x: x[1])[0]
        summary_parts.append(f"\nğŸ¯ æ ¸å¿ƒè­°é¡Œï¼šä½ æœ€é—œæ³¨çš„ä¸»é¡Œæ˜¯ã€Œ{top_topic}ã€")
        
        if top_topic == "å·¥ä½œè·å ´":
            summary_parts.append("å»ºè­°å»ºç«‹å·¥ä½œèˆ‡ç”Ÿæ´»çš„ç•Œç·šï¼Œé¿å…éåº¦æŠ•å…¥")
        elif top_topic == "äººéš›é—œä¿‚":
            summary_parts.append("äººéš›è­°é¡Œæ˜¯å¸¸è¦‹çš„å£“åŠ›æºï¼Œå­¸ç¿’é©ç•¶çš„æºé€šæŠ€å·§å¾ˆé‡è¦")
        elif top_topic == "è‡ªæˆ‘èªåŒ":
            summary_parts.append("è‡ªæˆ‘æ¢ç´¢æ˜¯æˆé•·çš„é‡è¦éç¨‹ï¼Œçµ¦è‡ªå·±æ™‚é–“æ…¢æ…¢é‡æ¸…")
    
    if trend_info and trend_info.get("trend") != "insufficient_data":
        summary_parts.append(f"\nğŸ“ˆ æƒ…ç·’è¶¨å‹¢ï¼š{trend_info.get('description', '')}")
    
    summary_parts.append("\nâœ¨ æŒçºŒèˆ‡ AI å¤¥ä¼´å°è©±ï¼Œæœ‰åŠ©æ–¼æ›´æ·±å…¥åœ°ç†è§£è‡ªå·±çš„æƒ…ç·’æ¨¡å¼")
    
    return "\n".join(summary_parts)

def analyze_chat_messages(messages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """å°ˆæ¥­ç‰ˆèŠå¤©è¨Šæ¯åˆ†æ - é–€æª»èª¿æ•´ç‚º30å¥"""
    
    user_messages = [m for m in messages if m.get("role") == "user"]
    
    if len(user_messages) < 30:
        return {
            "ok": False,
            "has_sufficient_data": False,
            "message_count": len(user_messages),
            "required_count": 30,
            "message": f"ç›®å‰å°è©±æ¬¡æ•¸ç‚º {len(user_messages)} æ¬¡ï¼Œè‡³å°‘éœ€è¦ 30 æ¬¡å°è©±æ‰èƒ½é€²è¡Œå®Œæ•´åˆ†æ"
        }
    
    emotion_frequency = Counter()
    emotion_intensity_sum = defaultdict(float)
    emotion_count = defaultdict(int)
    topic_scores = Counter()
    timeline_data = []
    
    now = datetime.now()
    
    for msg in user_messages:
        text = msg.get("content", "")
        created_at_str = msg.get("created_at")
        
        days_ago = 0
        if created_at_str:
            try:
                msg_date = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
                days_ago = (now - msg_date).days
            except:
                days_ago = 0
        
        emotions = detect_emotions_advanced(text, days_ago)
        
        if emotions:
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])[0]
            dominant_score = emotions[dominant_emotion]
            
            emotion_frequency[dominant_emotion] += 1
            
            for emotion, score in emotions.items():
                emotion_intensity_sum[emotion] += score
                emotion_count[emotion] += 1
            
            timeline_data.append({
                "date": created_at_str,
                "dominant_emotion": dominant_emotion,
                "score": dominant_score,
                "all_emotions": emotions
            })
        
        topics = detect_topics_advanced(text)
        for topic, score in topics.items():
            topic_scores[topic] += score
    
    emotion_intensity_avg = {
        emotion: emotion_intensity_sum[emotion] / emotion_count[emotion]
        for emotion in emotion_count
    }
    
    trend_info = analyze_emotion_trends(timeline_data)
    
    summary = generate_professional_summary(
        dict(emotion_frequency),
        emotion_intensity_avg,
        dict(topic_scores),
        len(user_messages),
        trend_info
    )
    
    topic_radar_data = {}
    for topic, score in topic_scores.most_common(6):
        topic_radar_data[topic] = round(score * 20, 1)

    return {
        "ok": True,
        "has_sufficient_data": True,
        "message_count": len(user_messages),
        "emotion_frequency": dict(emotion_frequency.most_common(7)),
        "emotion_intensity": {k: round(v * 100, 1) for k, v in emotion_intensity_avg.items()},
        "topic_radar": topic_radar_data,
        "timeline_data": timeline_data[-30:],
        "trend_analysis": trend_info,
        "summary": summary,
        "analysis_meta": {
            "analyzed_at": datetime.now().isoformat(),
            "period_start": user_messages[0].get("created_at") if user_messages else None,
            "period_end": user_messages[-1].get("created_at") if user_messages else None,
            "total_days": days_ago if user_messages else 0
        },
        "emotion_colors": {k: v["color"] for k, v in EMOTION_KEYWORDS.items()},
        "topic_colors": {k: v["color"] for k, v in TOPIC_KEYWORDS.items()}
    }